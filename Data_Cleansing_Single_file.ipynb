{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Locating Multivalued and Duplicate records (single csv file)\n",
    "### Saksham Gakhar, DA - DKSF\n",
    "\n",
    "Keep changing the input csv file and look for duplicate and multivalued records, enlist devices that generally misbehave..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "# without mpld3\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(dt):\n",
    "    \"\"\"\n",
    "        Read the CSV file into a dataframe for a YYYY-MM (dt)\n",
    "        arg: dt -- string with format YYYY-MM\n",
    "    \"\"\"\n",
    "    folder = '2020-07-06-DataKind/'\n",
    "    filename = 'output-' + str(dt) + '-01T00_00_00+00_00.csv'\n",
    "    df = pd.read_csv(folder+filename)\n",
    "    df.when_captured = pd.to_datetime(df.when_captured)\n",
    "\n",
    "    # Need to change the format of the Time Stamp for all the measurements in the raw data\n",
    "\n",
    "    df.service_uploaded =  df.service_uploaded.apply(lambda x: \\\n",
    "                            datetime.datetime.strptime(x, '%b %d, %Y @ %H:%M:%S.%f')\\\n",
    "                            .replace(tzinfo=datetime.timezone.utc))\n",
    "    #### Add a column for the year\n",
    "    df['year'] = pd.DatetimeIndex(df['when_captured']).year\n",
    "    df.info()\n",
    "    df[0:5]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above table for (`device`, `when_captured`) key, let's see what these multiple values for each time stamp correspond to. Sometimes there are negative RH, sometimes 0.0 PM (which measn very clean air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBadData(df):\n",
    "    \n",
    "    temp_df = df.groupby(['device_urn', 'device_sn','when_captured']).size().to_frame('size').\\\n",
    "                                    reset_index().sort_values('size', ascending=False)\n",
    "    print(\"bad device data counts: \")\n",
    "    badRecords = temp_df[(temp_df['size']>1)]\n",
    "    print(badRecords)\n",
    "    \n",
    "    print(\"all bad device list: \")\n",
    "    # Devices that have misbehaved at some point - more than one data values per time stamp\n",
    "    print(np.unique(temp_df[temp_df['size']>1]['device_sn'].values)) # devices that have misbehaved\n",
    "    \n",
    "    return badRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing based on [Protocol](https://github.com/DataKind-SF/safecast/blob/master/Solarcast_data_cleansing.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmInvalidTimeStamps(df):\n",
    "    \"\"\"\n",
    "    remove invalid time stamped records\n",
    "    \"\"\"\n",
    "    \n",
    "    ## remove records with NULL `when_captured`\n",
    "    print(\"Null date records to remove: \", df['when_captured'].isna().sum())\n",
    "    df = df[df['when_captured'].notna()]\n",
    "    print(\"df shape after remove records with NULL `when_captured` : \",df.shape)\n",
    "\n",
    "    ## remove records where `when_captured` is an invalid\n",
    "    boolean_condition = df['when_captured'] >  pd.to_datetime(2000/1/19, infer_datetime_format=True).tz_localize('UTC')\n",
    "    print(\"Valid `when_captured`  entires: \", boolean_condition.sum())\n",
    "    df = df[df['when_captured'] >  pd.to_datetime(2000/1/19, infer_datetime_format=True).tz_localize('UTC')]\n",
    "    print(\"df shape after remove records where `when_captured` is an invalid : \",df.shape)\n",
    "\n",
    "    ## remove records where gap of `service_uploaded` and `when_captured` > 7 days\n",
    "    boolean_condition = abs(df['when_captured'].subtract(df['service_uploaded'])).astype('timedelta64[D]') < 7\n",
    "    boolean_condition.shape\n",
    "    print(\"Lag 7 days to remove: \",df.shape[0] - (boolean_condition).sum())\n",
    "    df = df[boolean_condition]\n",
    "    print(\"df shape after records where gap of `service_uploaded` and `when_captured` > 7 days : \",df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeInaccurateRH(df):\n",
    "    \"\"\" \n",
    "    impute data with NaN(missing) for inaccurate values of RH\n",
    "    \"\"\"\n",
    "    \n",
    "    boolean_condition = (df['env_humid']<0) | (df['env_humid']>100)\n",
    "    column_name = 'env_humid'\n",
    "    new_value = np.nan\n",
    "    df.loc[boolean_condition, column_name] = new_value\n",
    "    print(\"Inaccurate RH records imputed: \", boolean_condition.sum())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropServiceUploaded(df):\n",
    "    \"\"\"\n",
    "    Inplace dropping of the 'service_uploaded' column\n",
    "    \"\"\"\n",
    "    df.drop('service_uploaded', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmDuplicates(df):\n",
    "    \"\"\"\n",
    "    Inplace dropping of duplicates\n",
    "    preserve a single copy of duplicative rows\n",
    "    \"\"\"\n",
    "    incoming = df.shape[0]\n",
    "    df.drop_duplicates(subset=df.columns[0:df.shape[1]], inplace=True, keep='first') # args: subset=[df.columns[0:df.shape[1]]], keep = 'first'\n",
    "    print(\"Number of duplicative entries removed : \", -df.shape[0]+incoming)\n",
    "\n",
    "# #testing inplace = True and no return in fucntion above\n",
    "# df = pd.DataFrame(np.arange(12).reshape(3, 4),columns=['A', 'B', 'C', 'D'])\n",
    "# df.loc[-1] = [0, 1, 2, 3] \n",
    "# df\n",
    "# rmDuplicates(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering bad row records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAggWithKey(df):\n",
    "    \"\"\"\n",
    "    Aggregate the df based on key: 'device_sn','when_captured'\n",
    "    arg: df - incoming dataframe\n",
    "    return: datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "    \"\"\"\n",
    "    # STEP 1: Aggregate the dataframe based on key\n",
    "    \n",
    "    temp_df = df.groupby(['device_sn','when_captured']).agg(['count','nunique'])\n",
    "    # temp_df.info()\n",
    "    num_groups = temp_df.shape[0]\n",
    "    print(\"num_groups  is : \", num_groups)\n",
    "\n",
    "    # STEP 2: Merge Counts and Count-Distincts to check for duplicative records and multiplicities\n",
    "\n",
    "    even = list(range(0,26,2))\n",
    "    odd = list(range(1,26,2))\n",
    "    tmp_df1 = temp_df.iloc[:,even].max(axis=1).to_frame('COUNTS').reset_index()\n",
    "    tmp_df2 = temp_df.iloc[:,odd].max(axis=1).to_frame('DISTINCTS').reset_index()\n",
    "    print(tmp_df1.shape, tmp_df2.shape)\n",
    "    merged = pd.merge(tmp_df1, tmp_df2, left_on = ['device_sn', 'when_captured'], \\\n",
    "                      right_on=['device_sn', 'when_captured'])\n",
    "    merged.head\n",
    "    return merged, num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating hits: Impose mutually exclusive conditions for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyALLNanRecs(merged):\n",
    "    \"\"\"\n",
    "        Actionable: Records of useless data with all NaNs\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "        return : keys dataframe ('device_sn', 'when_captured') to remove later\n",
    "    \"\"\"\n",
    "    bool1 = (merged.COUNTS >1) & (merged.DISTINCTS==0)\n",
    "    sum1 = bool1.sum()\n",
    "    print(sum1)\n",
    "    toDiscard1 = merged.loc[:,['device_sn', 'when_captured']][bool1]\n",
    "    toDiscard1.shape\n",
    "    return sum1, toDiscard1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyMultivaluedTimeStamps(merged):\n",
    "    \"\"\"\n",
    "        Actionable: Records that are a mix of duplicates and non-duplicate rows \n",
    "        for a given (`device_sn`, `when_captured`) [must be all discarded]\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "        return : keys dataframe ('device_sn', 'when_captured') to remove later\n",
    "    \"\"\"\n",
    "    bool3 = (merged.COUNTS >1) & (merged.DISTINCTS>1)\n",
    "    sum3 = bool3.sum()\n",
    "    print(sum3)\n",
    "    toDiscard3 = merged.loc[:,['device_sn', 'when_captured']][bool3]\n",
    "    toDiscard3.shape\n",
    "    return sum3, toDiscard3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyRemainingDupl(merged):\n",
    "    \"\"\"\n",
    "        NOT Actionable as duplicates were dropped: \n",
    "        Records where all rows are purely duplicates [preserve only 1 later]\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "    \"\"\"\n",
    "    bool2 = (merged.COUNTS >1) & (merged.DISTINCTS==1)\n",
    "    sum2 = bool2.sum()\n",
    "    print(\"remaining duplicates check : \" ,merged.COUNTS[bool2].sum() - merged.DISTINCTS[bool2].sum())\n",
    "    return sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodTimeStamps(merged):\n",
    "    \"\"\"\n",
    "        Records that are good\n",
    "    \"\"\"\n",
    "    bool4 = (merged.COUNTS ==1) & (merged.DISTINCTS==1)\n",
    "    sum4 = bool4.sum()\n",
    "    print('good records : ', sum4)\n",
    "    return sum4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDF(dframe, descrpt):\n",
    "    \"\"\"\n",
    "        write multivalued timestamps' keys to a csv\n",
    "        args: dframe to write\n",
    "        descrpt: string with descripttion to append to file\n",
    "    \"\"\"\n",
    "    dframe.info()\n",
    "    print(\"written records count : \", dframe.shape[0])\n",
    "    dframe.to_csv(str(dt) + '-01_anomalies_' + str(descrpt) + '_.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard bas data now from the main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRows(toDiscard1, toDiscard3, df):\n",
    "    \"\"\"\n",
    "        Inplace discarding of rows based on allNaN record keys (in df : toDiscard1)\n",
    "        and rows based on MultivaluedTimeStamps keys (in df : toDiscard3)\n",
    "        from original dataframe: df\n",
    "        args:\n",
    "            toDiscard1: allNaN record keys\n",
    "            toDiscard3: MultivaluedTimeStamps keys\n",
    "            df: original dataframe\n",
    "    \"\"\"\n",
    "    # STEP 1 : \n",
    "    # all tuples of keys to be discarded\n",
    "    discard = pd.concat([toDiscard1, toDiscard3], ignore_index=True)\n",
    "    discard['KEY_DevSN_WhenCapt'] = list(zip(discard.device_sn, discard.when_captured))\n",
    "    print(df.shape, discard.shape)\n",
    "\n",
    "    # STEP 2 :\n",
    "    # tuples of all keys in the dataframe\n",
    "    df['KEY_DevSN_WhenCapt'] = list(zip(df.device_sn, df.when_captured))\n",
    "    df.shape\n",
    "\n",
    "    # STEP 3 : \n",
    "    # discard the rows\n",
    "    rows_to_discard = df['KEY_DevSN_WhenCapt'].isin(discard['KEY_DevSN_WhenCapt'])\n",
    "    print(\"these many rows to discard: \", rows_to_discard.sum())\n",
    "\n",
    "    incoming = df.shape[0]\n",
    "    df = df[~rows_to_discard]\n",
    "    print(incoming - df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run cleaning algo functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2017-09-01','2020-07-01', freq='MS').strftime(\"%Y-%m\").tolist()\n",
    "dt = dates[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64655 entries, 0 to 64654\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   service_uploaded  64655 non-null  datetime64[ns, UTC]\n",
      " 1   when_captured     61839 non-null  datetime64[ns, UTC]\n",
      " 2   device_urn        64655 non-null  object             \n",
      " 3   device_sn         64655 non-null  object             \n",
      " 4   device            64655 non-null  int64              \n",
      " 5   loc_lat           64655 non-null  float64            \n",
      " 6   loc_lon           64655 non-null  float64            \n",
      " 7   env_temp          36580 non-null  float64            \n",
      " 8   env_humid         36580 non-null  float64            \n",
      " 9   pms_pm01_0        36899 non-null  object             \n",
      " 10  pms_pm02_5        36898 non-null  object             \n",
      " 11  pms_pm10_0        36897 non-null  object             \n",
      " 12  lnd_7318c         55230 non-null  float64            \n",
      " 13  lnd_7318u         55231 non-null  float64            \n",
      " 14  bat_voltage       24201 non-null  float64            \n",
      " 15  year              61839 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](2), float64(8), int64(1), object(5)\n",
      "memory usage: 7.9+ MB\n",
      "bad device data counts: \n",
      "                device_urn         device_sn             when_captured  size\n",
      "5634    safecast:114699387  Solarcast #30023 2017-09-11 08:30:04+00:00     5\n",
      "5639    safecast:114699387  Solarcast #30023 2017-09-11 14:45:03+00:00     5\n",
      "5635    safecast:114699387  Solarcast #30023 2017-09-11 09:45:04+00:00     5\n",
      "5636    safecast:114699387  Solarcast #30023 2017-09-11 11:00:03+00:00     5\n",
      "5637    safecast:114699387  Solarcast #30023 2017-09-11 12:15:03+00:00     5\n",
      "5638    safecast:114699387  Solarcast #30023 2017-09-11 13:30:03+00:00     5\n",
      "5633    safecast:114699387  Solarcast #30023 2017-09-11 06:00:04+00:00     5\n",
      "5641    safecast:114699387  Solarcast #30023 2017-09-11 16:00:03+00:00     3\n",
      "31971  safecast:3714913954  Solarcast #30027 2017-09-29 09:22:50+00:00     2\n",
      "32368  safecast:3714913954  Solarcast #30027 2017-09-30 13:52:50+00:00     2\n",
      "32417  safecast:3714913954  Solarcast #30027 2017-09-30 17:22:50+00:00     2\n",
      "31495  safecast:3714913954  Solarcast #30027 2017-09-28 10:47:38+00:00     2\n",
      "32382  safecast:3714913954  Solarcast #30027 2017-09-30 14:52:50+00:00     2\n",
      "28608  safecast:3714913954  Solarcast #30027 2017-09-21 09:25:23+00:00     2\n",
      "28780  safecast:3714913954  Solarcast #30027 2017-09-21 16:05:38+00:00     2\n",
      "32187  safecast:3714913954  Solarcast #30027 2017-09-30 00:52:50+00:00     2\n",
      "all bad device list: \n",
      "['Solarcast #30023' 'Solarcast #30027']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_urn</th>\n",
       "      <th>device_sn</th>\n",
       "      <th>when_captured</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 08:30:04+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 14:45:03+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 09:45:04+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5636</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 11:00:03+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5637</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 12:15:03+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 13:30:03+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 06:00:04+00:00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>safecast:114699387</td>\n",
       "      <td>Solarcast #30023</td>\n",
       "      <td>2017-09-11 16:00:03+00:00</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-29 09:22:50+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32368</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-30 13:52:50+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32417</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-30 17:22:50+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31495</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-28 10:47:38+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32382</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-30 14:52:50+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28608</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-21 09:25:23+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28780</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-21 16:05:38+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32187</th>\n",
       "      <td>safecast:3714913954</td>\n",
       "      <td>Solarcast #30027</td>\n",
       "      <td>2017-09-30 00:52:50+00:00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                device_urn         device_sn             when_captured  size\n",
       "5634    safecast:114699387  Solarcast #30023 2017-09-11 08:30:04+00:00     5\n",
       "5639    safecast:114699387  Solarcast #30023 2017-09-11 14:45:03+00:00     5\n",
       "5635    safecast:114699387  Solarcast #30023 2017-09-11 09:45:04+00:00     5\n",
       "5636    safecast:114699387  Solarcast #30023 2017-09-11 11:00:03+00:00     5\n",
       "5637    safecast:114699387  Solarcast #30023 2017-09-11 12:15:03+00:00     5\n",
       "5638    safecast:114699387  Solarcast #30023 2017-09-11 13:30:03+00:00     5\n",
       "5633    safecast:114699387  Solarcast #30023 2017-09-11 06:00:04+00:00     5\n",
       "5641    safecast:114699387  Solarcast #30023 2017-09-11 16:00:03+00:00     3\n",
       "31971  safecast:3714913954  Solarcast #30027 2017-09-29 09:22:50+00:00     2\n",
       "32368  safecast:3714913954  Solarcast #30027 2017-09-30 13:52:50+00:00     2\n",
       "32417  safecast:3714913954  Solarcast #30027 2017-09-30 17:22:50+00:00     2\n",
       "31495  safecast:3714913954  Solarcast #30027 2017-09-28 10:47:38+00:00     2\n",
       "32382  safecast:3714913954  Solarcast #30027 2017-09-30 14:52:50+00:00     2\n",
       "28608  safecast:3714913954  Solarcast #30027 2017-09-21 09:25:23+00:00     2\n",
       "28780  safecast:3714913954  Solarcast #30027 2017-09-21 16:05:38+00:00     2\n",
       "32187  safecast:3714913954  Solarcast #30027 2017-09-30 00:52:50+00:00     2"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = readCSV(dt)\n",
    "findBadData(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null date records to remove:  2816\n",
      "df shape after remove records with NULL `when_captured` :  (61839, 16)\n",
      "Valid `when_captured`  entires:  61839\n",
      "df shape after remove records where `when_captured` is an invalid :  (61839, 16)\n",
      "Lag 7 days to remove:  0\n",
      "df shape after records where gap of `service_uploaded` and `when_captured` > 7 days :  (61839, 16)\n",
      "new df:  (61839, 16)\n"
     ]
    }
   ],
   "source": [
    "df = rmInvalidTimeStamps(df)\n",
    "print(\"new df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaccurate RH records imputed:  9102\n",
      "new df:  (61839, 16)\n"
     ]
    }
   ],
   "source": [
    "df = imputeInaccurateRH(df)\n",
    "print(\"new df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new df:  (61839, 15)\n"
     ]
    }
   ],
   "source": [
    "dropServiceUploaded(df)\n",
    "print(\"new df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicative entries removed :  8\n",
      "new df:  (61831, 15)\n"
     ]
    }
   ],
   "source": [
    "rmDuplicates(df)\n",
    "print(\"new df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_groups  is :  61801\n",
      "(61801, 3) (61801, 3)\n",
      "merged:  (61801, 4)\n",
      "num_groups :  61801\n"
     ]
    }
   ],
   "source": [
    "merged,num_groups = dataAggWithKey(df)\n",
    "print(\"merged: \", merged.shape)\n",
    "print(\"num_groups : \", num_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "8\n",
      "remaining duplicates check :  0\n",
      "good records :  61793\n",
      "toDiscard1 shape:  (0, 2)\n",
      "toDiscard3 shape:  (8, 2)\n"
     ]
    }
   ],
   "source": [
    "sum1, toDiscard1 = identifyALLNanRecs(merged)\n",
    "sum3, toDiscard3 = identifyMultivaluedTimeStamps(merged)\n",
    "sum2 = identifyRemainingDupl(merged)\n",
    "sum4 = goodTimeStamps(merged)\n",
    "print(\"toDiscard1 shape: \",toDiscard1.shape)\n",
    "print(\"toDiscard3 shape: \",toDiscard3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanityCheck(): ensure you have all records covered by 1 of the 4 conditions\n",
    "assert(num_groups == sum1+sum2+sum3+sum4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8 entries, 40839 to 40847\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   device_sn      8 non-null      object             \n",
      " 1   when_captured  8 non-null      datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), object(1)\n",
      "memory usage: 192.0+ bytes\n",
      "written records count :  8\n"
     ]
    }
   ],
   "source": [
    "writeDF(toDiscard3, 'MultivaluedTimeStamps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61831, 15) (8, 3)\n",
      "these many rows to discard:  38\n",
      "38\n",
      "final df shape:  (61793, 16)\n"
     ]
    }
   ],
   "source": [
    "df = filterRows(toDiscard1, toDiscard3, df)\n",
    "print(\"final df shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now check to make sure no garbage data is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad device data counts: \n",
      "Empty DataFrame\n",
      "Columns: [device_urn, device_sn, when_captured, size]\n",
      "Index: []\n",
      "all bad device list: \n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "badRecordsLeft = findBadData(df)\n",
    "badRecordsLeft\n",
    "assert(badRecordsLeft.empty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
