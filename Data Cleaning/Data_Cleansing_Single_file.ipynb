{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Locating Multivalued and Duplicate records (single csv file)\n",
    "### Saksham Gakhar, DA - DKSF\n",
    "\n",
    "Keep changing the input csv file and look for duplicate and multivalued records, enlist devices that generally misbehave..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "# without mpld3\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(dt):\n",
    "    \"\"\"\n",
    "        Read the CSV file into a dataframe for a YYYY-MM (dt)\n",
    "        Do preliminary cleaning\n",
    "        arg: dt -- string with format YYYY-MM\n",
    "        return df: dataframe containing data from csv\n",
    "    \"\"\"\n",
    "    folder = '2020-07-06-DataKind/'\n",
    "    filename = 'output-' + str(dt) + '-01T00_00_00+00_00.csv'\n",
    "    df = pd.read_csv(folder+filename)\n",
    "    df.when_captured = pd.to_datetime(df.when_captured)\n",
    "\n",
    "    # Need to change the format of the Time Stamp for all the measurements in the raw data\n",
    "\n",
    "    df.service_uploaded =  df.service_uploaded.apply(lambda x: \\\n",
    "                            datetime.datetime.strptime(x, '%b %d, %Y @ %H:%M:%S.%f')\\\n",
    "                            .replace(tzinfo=datetime.timezone.utc))\n",
    "    #### Add a column for the year\n",
    "    df['year'] = pd.DatetimeIndex(df['when_captured']).year\n",
    "    \n",
    "    #### Need to correct for the format of the PM numeric values. \n",
    "    df['pms_pm01_0'] = df['pms_pm01_0'].astype(str).str.replace(',', '').astype(float)\n",
    "    df['pms_pm10_0'] = df['pms_pm10_0'].astype(str).str.replace(',', '').astype(float)\n",
    "    df['pms_pm02_5'] = df['pms_pm02_5'].astype(str).str.replace(',', '').astype(float)\n",
    "    \n",
    "    df.info()\n",
    "    df[0:5]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above table for (`device`, `when_captured`) key, let's see what these multiple values for each time stamp correspond to. Sometimes there are negative RH, sometimes 0.0 PM (which measn very clean air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBadData(df):\n",
    "    \n",
    "    temp_df = df.groupby(['device_urn', 'device_sn','when_captured']).size().to_frame('size').\\\n",
    "                                    reset_index().sort_values('size', ascending=False)\n",
    "    print(\"bad device data counts: \")\n",
    "    badRecords = temp_df[(temp_df['size']>1)]\n",
    "    print(badRecords)\n",
    "    \n",
    "    print(\"all bad device list: \")\n",
    "    # Devices that have misbehaved at some point - more than one data values per time stamp\n",
    "    print(np.unique(temp_df[temp_df['size']>1]['device_sn'].values)) # devices that have misbehaved\n",
    "    \n",
    "    return badRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing based on [Protocol](https://github.com/DataKind-SF/safecast/blob/master/Solarcast_data_cleansing.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmInvalidTimeStamps(df):\n",
    "    \"\"\"\n",
    "    remove invalid time stamped records\n",
    "    \"\"\"\n",
    "    \n",
    "    ## remove records with NULL `when_captured`\n",
    "    print(\"Null date records to remove: \", df['when_captured'].isna().sum())\n",
    "    df = df[df['when_captured'].notna()]\n",
    "    print(\"df shape after remove records with NULL `when_captured` : \",df.shape)\n",
    "\n",
    "    ## remove records where `when_captured` is an invalid\n",
    "    boolean_condition = df['when_captured'] >  pd.to_datetime(2000/1/19, infer_datetime_format=True).tz_localize('UTC')\n",
    "    print(\"Valid `when_captured`  entires: \", boolean_condition.sum())\n",
    "    df = df[df['when_captured'] >  pd.to_datetime(2000/1/19, infer_datetime_format=True).tz_localize('UTC')]\n",
    "    print(\"df shape after remove records where `when_captured` is an invalid : \",df.shape)\n",
    "\n",
    "    ## remove records where gap of `service_uploaded` and `when_captured` > 7 days\n",
    "    boolean_condition = abs(df['when_captured'].subtract(df['service_uploaded'])).astype('timedelta64[D]') < 7\n",
    "    boolean_condition.shape\n",
    "    print(\"Lag 7 days to remove: \",df.shape[0] - (boolean_condition).sum())\n",
    "    df = df[boolean_condition]\n",
    "    print(\"df shape after records where gap of `service_uploaded` and `when_captured` > 7 days : \",df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputeInaccurateRH(df):\n",
    "    \"\"\" \n",
    "    impute data with NaN(missing) for inaccurate values of RH\n",
    "    \"\"\"\n",
    "    \n",
    "    boolean_condition = (df['env_humid']<0) | (df['env_humid']>100)\n",
    "    column_name = 'env_humid'\n",
    "    new_value = np.nan\n",
    "    df.loc[boolean_condition, column_name] = new_value\n",
    "    print(\"Inaccurate RH records imputed: \", boolean_condition.sum())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropServiceUploaded(df):\n",
    "    \"\"\"\n",
    "    Inplace dropping of the 'service_uploaded' column\n",
    "    \"\"\"\n",
    "    df.drop('service_uploaded', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmDuplicates(df):\n",
    "    \"\"\"\n",
    "    Inplace dropping of duplicates\n",
    "    preserve a single copy of duplicative rows\n",
    "    \"\"\"\n",
    "    incoming = df.shape[0]\n",
    "    df.drop_duplicates(subset=df.columns[0:df.shape[1]], inplace=True, keep='first') # args: subset=[df.columns[0:df.shape[1]]], keep = 'first'\n",
    "    print(\"Number of duplicative entries removed : \", -df.shape[0]+incoming)\n",
    "\n",
    "# #testing inplace = True and no return in fucntion above\n",
    "# df = pd.DataFrame(np.arange(12).reshape(3, 4),columns=['A', 'B', 'C', 'D'])\n",
    "# df.loc[-1] = [0, 1, 2, 3] \n",
    "# df\n",
    "# rmDuplicates(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering bad row records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAggWithKey(df):\n",
    "    \"\"\"\n",
    "    Aggregate the df based on key: 'device_sn','when_captured'\n",
    "    arg: df - incoming dataframe\n",
    "    return: datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "    \"\"\"\n",
    "    # STEP 1: Aggregate the dataframe based on key\n",
    "    \n",
    "    temp_df = df.groupby(['device_sn','when_captured']).agg(['count','nunique'])\n",
    "    # temp_df.info()\n",
    "    num_groups = temp_df.shape[0]\n",
    "    print(\"num_groups  is : \", num_groups)\n",
    "\n",
    "    # STEP 2: Merge Counts and Count-Distincts to check for duplicative records and multiplicities\n",
    "\n",
    "    even = list(range(0,26,2))\n",
    "    odd = list(range(1,26,2))\n",
    "    tmp_df1 = temp_df.iloc[:,even].max(axis=1).to_frame('COUNTS').reset_index()\n",
    "    tmp_df2 = temp_df.iloc[:,odd].max(axis=1).to_frame('DISTINCTS').reset_index()\n",
    "    print(tmp_df1.shape, tmp_df2.shape)\n",
    "    merged = pd.merge(tmp_df1, tmp_df2, left_on = ['device_sn', 'when_captured'], \\\n",
    "                      right_on=['device_sn', 'when_captured'])\n",
    "    merged.head\n",
    "    return merged, num_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating hits: Impose mutually exclusive conditions for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyALLNanRecs(merged):\n",
    "    \"\"\"\n",
    "        Actionable: Records of useless data with all NaNs\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "        return : keys dataframe ('device_sn', 'when_captured') to remove later\n",
    "    \"\"\"\n",
    "    bool1 = (merged.COUNTS >1) & (merged.DISTINCTS==0)\n",
    "    sum1 = bool1.sum()\n",
    "    print(sum1)\n",
    "    toDiscard1 = merged.loc[:,['device_sn', 'when_captured']][bool1]\n",
    "    toDiscard1.shape\n",
    "    return sum1, toDiscard1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyMultivaluedTimeStamps(merged):\n",
    "    \"\"\"\n",
    "        Actionable: Records that are a mix of duplicates and non-duplicate rows \n",
    "        for a given (`device_sn`, `when_captured`) [must be all discarded]\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "        return : keys dataframe ('device_sn', 'when_captured') to remove later\n",
    "    \"\"\"\n",
    "    bool3 = (merged.COUNTS >1) & (merged.DISTINCTS>1)\n",
    "    sum3 = bool3.sum()\n",
    "    print(sum3)\n",
    "    toDiscard3 = merged.loc[:,['device_sn', 'when_captured']][bool3]\n",
    "    toDiscard3.shape\n",
    "    return sum3, toDiscard3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyRemainingDupl(merged):\n",
    "    \"\"\"\n",
    "        NOT Actionable as duplicates were dropped: \n",
    "        Records where all rows are purely duplicates [preserve only 1 later]\n",
    "        args: incoming datframe with COUNTS and COUNT-DISTINCTS for each key\n",
    "    \"\"\"\n",
    "    bool2 = (merged.COUNTS >1) & (merged.DISTINCTS==1)\n",
    "    sum2 = bool2.sum()\n",
    "    print(\"remaining duplicates check : \" ,merged.COUNTS[bool2].sum() - merged.DISTINCTS[bool2].sum())\n",
    "    return sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodTimeStamps(merged):\n",
    "    \"\"\"\n",
    "        Records that are good\n",
    "    \"\"\"\n",
    "    bool4 = (merged.COUNTS ==1) & (merged.DISTINCTS==1)\n",
    "    sum4 = bool4.sum()\n",
    "    print('good records : ', sum4)\n",
    "    return sum4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDF(dt, dframe, descrpt):\n",
    "    \"\"\"\n",
    "        write multivalued timestamps' keys to a csv\n",
    "        args: dframe to write\n",
    "        descrpt: string with descripttion to append to file\n",
    "    \"\"\"\n",
    "    # dframe.info()\n",
    "    print(\"written records shape : \", dframe.shape)\n",
    "    dframe.to_csv(str(dt) + '-01_' + str(descrpt) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard bad data now from the main dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterRows(toDiscard1, toDiscard3, df):\n",
    "    \"\"\"\n",
    "        Inplace discarding of rows based on allNaN record keys (in df : toDiscard1)\n",
    "        and rows based on MultivaluedTimeStamps keys (in df : toDiscard3)\n",
    "        from original dataframe: df\n",
    "        args:\n",
    "            toDiscard1: allNaN record keys\n",
    "            toDiscard3: MultivaluedTimeStamps keys\n",
    "            df: original dataframe\n",
    "    \"\"\"\n",
    "    # STEP 1 : \n",
    "    # all tuples of keys to be discarded\n",
    "    discard = pd.concat([toDiscard1, toDiscard3], ignore_index=True)\n",
    "    discard['KEY_DevSN_WhenCapt'] = list(zip(discard.device_sn, discard.when_captured))\n",
    "    print(df.shape, discard.shape)\n",
    "\n",
    "    # STEP 2 :\n",
    "    # tuples of all keys in the dataframe\n",
    "    df['KEY_DevSN_WhenCapt'] = list(zip(df.device_sn, df.when_captured))\n",
    "    df.shape\n",
    "\n",
    "    # STEP 3 : \n",
    "    # discard the rows\n",
    "    rows_to_discard = df['KEY_DevSN_WhenCapt'].isin(discard['KEY_DevSN_WhenCapt'])\n",
    "    print(\"these many rows to discard: \", rows_to_discard.sum())\n",
    "\n",
    "    incoming = df.shape[0]\n",
    "    df = df[~rows_to_discard]\n",
    "    print(incoming - df.shape[0])\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
